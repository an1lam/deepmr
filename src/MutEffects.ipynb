{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kipoi\n",
    "# !pip install kipoiseq\n",
    "# !pip install pybedtools\n",
    "# !pip uninstall -y kipoi_veff\n",
    "# !pip install git+https://github.com/an1lam/kipoi-veff\n",
    "# !pip install pyvcf\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import kipoi\n",
    "from kipoi_interpret.importance_scores.ism import Mutation\n",
    "from kipoiseq.dataloaders import SeqIntervalDl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stephenmalina/project/src\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DNA sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "dl = SeqIntervalDl(\"../dat/50_random_seqs_2.bed\", \"../dat/hg19.fa\", auto_resize_len=1000)\n",
    "data = dl.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4, 1, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = np.expand_dims(data['inputs'].transpose(0, 2, 1), 2).astype(np.float32)\n",
    "seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DeepSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.14.0\n",
      "torch version: 1.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = kipoi.list_models()\n",
    "# deepsea_models = df[df.model.str.contains(\"DeepSEA\")]\n",
    "# deepsea_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/stephenmalina/.kipoi/models/DeepSEA/predict/downloaded/model_files/weights/89e640bf6bdbe1ff165f484d9796efc7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ReCodeAlphabet()\n",
       "  (1): ConcatenateRC()\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(4, 320, kernel_size=(1, 8), stride=(1, 1))\n",
       "    (1): Threshold(threshold=0, value=1e-06)\n",
       "    (2): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Conv2d(320, 480, kernel_size=(1, 8), stride=(1, 1))\n",
       "    (5): Threshold(threshold=0, value=1e-06)\n",
       "    (6): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv2d(480, 960, kernel_size=(1, 8), stride=(1, 1))\n",
       "    (9): Threshold(threshold=0, value=1e-06)\n",
       "    (10): Dropout(p=0.5, inplace=False)\n",
       "    (11): Lambda()\n",
       "    (12): Sequential(\n",
       "      (0): Lambda()\n",
       "      (1): Linear(in_features=50880, out_features=925, bias=True)\n",
       "    )\n",
       "    (13): Threshold(threshold=0, value=1e-06)\n",
       "    (14): Sequential(\n",
       "      (0): Lambda()\n",
       "      (1): Linear(in_features=925, out_features=919, bias=True)\n",
       "    )\n",
       "    (15): Sigmoid()\n",
       "  )\n",
       "  (3): AverageRC()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsea = kipoi.get_model(\"DeepSEA/predict\", source=\"kipoi\")\n",
    "deepsea.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROM_ACC_COL = 'HepG2_DNase_None'\n",
    "# TF_COL = 'A549_CTCF_None'\n",
    "TF_COL = 'HepG2_FOXA1_None'\n",
    "relevant_cols = sorted([(i, label)\n",
    "                        for i, label in enumerate(deepsea.schema.targets.column_labels)\n",
    "                        if label in [CHROM_ACC_COL, TF_COL]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 919)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsea.pipeline.predict_example().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle_file = \"../dat/most_recent_sat_mut_results__drop_channel.pickle\"\n",
    "pickle_file = \"../dat/most_recent_sat_mut_results__original_mc_dropout.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 10, 301, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 50, 10, 301, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pickle_file, 'rb') as f: np_preds = pickle.load(f)\n",
    "print(np_preds.shape)\n",
    "epochs, n_seqs, n_batches, batch_size, _ = np_preds.shape\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zeros = np.zeros((4,))\n",
    "\n",
    "def batches_needed(seq_len, batch_size, alpha_size=4):\n",
    "    assert ((seq_len * (alpha_size-1)) % (batch_size-1)) == 0, seq_len * 3\n",
    "    # alpha_size - 1 mutations per nt and then account for ref in each batch\n",
    "    return (seq_len * (alpha_size-1)) // (batch_size-1)\n",
    "\n",
    "def generate_wt_mut_batches(seq, batch_size):\n",
    "    \"\"\"\n",
    "    For a given sequence, generate all possible point-mutated versions of the sequence\n",
    "    in batches of size `param:batch_size`.\n",
    "    \n",
    "    Args:\n",
    "        seq (numpy.ndarray [number of base pairs, sequence length]): \n",
    "            wild type sequence.\n",
    "        batch_size (int): size of returned batches. Note that each batch will have the\n",
    "            wild type sequence as its first row since we need to compute wild type / mut\n",
    "            prediction diffs using predictions generated by the same dropout mask.\n",
    "    \"\"\"\n",
    "    num_nts, seq_len = seq.shape\n",
    "    n_batches = batches_needed(seq_len, batch_size, alpha_size=num_nts)\n",
    "    seq_batch = seq[np.newaxis, :, :].repeat(batch_size, axis=0)\n",
    "    seq_batches = seq_batch[np.newaxis, :, :, :].repeat(n_batches, axis=0)\n",
    "    i = 0\n",
    "    for seq_idx in range(seq_len):  # iterate over sequence \n",
    "        for nt_idx in range(num_nts):  # iterate over nucleotides\n",
    "            curr_batch, curr_idx = i // (batch_size - 1), (i % (batch_size-1) + 1)\n",
    "            \n",
    "            curr_nt = seq[nt_idx, seq_idx]\n",
    "            if int(curr_nt) == 1: continue\n",
    "\n",
    "            seq_batches[curr_batch, curr_idx, :, seq_idx] = all_zeros\n",
    "            seq_batches[curr_batch, curr_idx, nt_idx, seq_idx] = 1\n",
    "            i += 1\n",
    "    return seq_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_seq(it):\n",
    "    return (np\n",
    "            .expand_dims(next(it)[\"inputs\"].transpose(0, 2, 1), 2)\n",
    "            .astype(np.float32)\n",
    "            .squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 50 seqs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 764.49it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs, n_seqs, batch_size = 50, 50, 301\n",
    "n_nts, _, seq_len = deepsea.schema.inputs.shape\n",
    "preds = [[[] for _ in range(n_seqs)] for _ in range(epochs)]\n",
    "it = dl.batch_iter(batch_size=1, num_workers=0, drop_last=False)\n",
    "\n",
    "print(f\"Generating predictions for {len(it)} seqs\")\n",
    "n_batches = batches_needed(seq_len, batch_size, alpha_size=n_nts)\n",
    "seqs = np.zeros((n_seqs, n_nts, seq_len))\n",
    "batch_size = 301\n",
    "for i in tqdm(range(min(n_seqs, len(it)))):\n",
    "    seq = next_seq(it)\n",
    "    if np.allclose(seq, .25): raise Exception(\"shouldn't have empty seqs\")\n",
    "    seqs[i, :, :] = seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Analysis\n",
    "## Computing summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_uniform_prob = math.log(.05/(1-.05))\n",
    "def compute_normalized_prob(prob, train_prob):\n",
    "    # source: http://deepsea.princeton.edu/help/\n",
    "    denom = 1+np.exp(-(np.log(prob/(1-prob))+log_uniform_prob-np.log(train_prob/(1-train_prob))))\n",
    "    return 1 / denom\n",
    "\n",
    "# Ratios and normalization formula drawn from here: http://deepsea.princeton.edu/media/help/posproportion.txt\n",
    "tf_compute_normalized_prob = lambda prob: compute_normalized_prob(prob, 0.02394)\n",
    "chrom_acc_normalized_prob = lambda prob: compute_normalized_prob(prob, 0.049791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_preds[:, :, :, :, 0] = chrom_acc_normalized_prob(np_preds[:, :, :, :, 0])\n",
    "np_preds[:, :, :, :, 1] = compute_normalized_prob(np_preds[:, :, :, :, 1], 0.020508)\n",
    "np_preds[:, :, :, :, 2] = compute_normalized_prob(np_preds[:, :, :, :, 2], 0.02394)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 10, 301, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches = np_preds.shape[2]\n",
    "batch_size = np_preds.shape[3]\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10, 301, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_pred_means = np.mean(np_preds[:, :, :, :, :], axis=0)\n",
    "np_pred_vars = np.var(np_preds, axis=0, dtype=np.float64)\n",
    "np_pred_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10, 300, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_pred_mean_diffs = np_pred_means[:, :, 1:, :] - np_pred_means[:, :, 0:1, :] \n",
    "np_pred_mean_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_covs = np.zeros((n_seqs, n_batches, batch_size, 2, 2, len(relevant_cols)))\n",
    "for seq in range(n_seqs):\n",
    "    for batch in range(n_batches):\n",
    "        for col in range(len(relevant_cols)):\n",
    "            ref_seq_preds = np_preds[:, seq, batch, 0, col]\n",
    "            for mut in range(batch_size):\n",
    "                mut_seq_preds = np_preds[:, seq, batch, mut, col]\n",
    "                cov = np.cov(np.stack((ref_seq_preds, mut_seq_preds)), ddof=0) # 2x2, symmetric\n",
    "                np_pred_covs[seq, batch, mut, :, :, col] = cov # off diag idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_uncertainties = np.sqrt(\n",
    "    np_pred_covs[:, :, 1:, 1, 1, :] + np_pred_covs[:, :, 1:, 0, 0, :] - 2 * np_pred_covs[:, :, 1:, 0, 1, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most impactful mutations\n",
    "Because we're lazy and it's only 3,000 items, we're going to just sort the entire list of mutations by the absolute value of their predicted impact and then return the top 10.\n",
    "\n",
    "Once we do that, we're going to display logos for the impact of each one and its surrounding sequence context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "MutEffect = namedtuple('MutEffect', ['eff', 'seq_idx', 'eff_idx'])\n",
    "\n",
    "def rank_mutations_by_impact(seq, mut_effects, batch_size, feature_col=1):\n",
    "    \"\"\"\n",
    "    For a given sequence, generate all possible point-mutated versions of the sequence\n",
    "    in batches of size `param:batch_size`.\n",
    "    \n",
    "    Args:\n",
    "        seq (numpy.ndarray [number of base pairs, sequence length]): \n",
    "            wild type sequence.\n",
    "        batch_size (int): size of returned batches. Note that each batch will have the\n",
    "            wild type sequence as its first row since we need to compute wild type / mut\n",
    "            prediction diffs using predictions generated by the same dropout mask.\n",
    "    \"\"\"\n",
    "    num_nts, seq_len = seq.shape\n",
    "    n_batches = batches_needed(seq_len, batch_size, alpha_size=num_nts)\n",
    "    mut_impacts_by_pos = []\n",
    "    i = 0\n",
    "    for seq_idx in range(seq_len):  # iterate over sequence \n",
    "        for nt_idx in range(num_nts):  # iterate over nucleotides\n",
    "            curr_batch, curr_idx = i // (batch_size - 1), (i % (batch_size-1))\n",
    "            curr_nt = seq[nt_idx, seq_idx]\n",
    "            if int(curr_nt) == 1: continue\n",
    "\n",
    "            \n",
    "            mut_impacts_by_pos.append(\n",
    "                MutEffect(\n",
    "                    eff=mut_effects[curr_batch, curr_idx, feature_col],\n",
    "                    seq_idx=(seq_idx, nt_idx),\n",
    "                    eff_idx=(curr_batch, curr_idx)\n",
    "                )\n",
    "            )            \n",
    "\n",
    "            i += 1\n",
    "    return sorted(mut_impacts_by_pos, key=lambda me: abs(me.eff), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_mut_effects = rank_mutations_by_impact(seqs[2, :, :], np_pred_mean_diffs[2], batch_size)\n",
    "seq = seqs[2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation - effect: -0.26, sequence position: 554, from/to: C/A\n",
      "GTAAA * C/A * CATCA\n",
      "\n",
      "Mutation - effect: -0.26, sequence position: 549, from/to: G/T\n",
      "AAAAA * G/T * GTAAA\n",
      "\n",
      "Mutation - effect: 0.25, sequence position: 616, from/to: C/T\n",
      "CTTTG * C/T * CTTAC\n",
      "\n",
      "Mutation - effect: -0.25, sequence position: 553, from/to: A/T\n",
      "AGTAA * A/T * ACATC\n",
      "\n",
      "Mutation - effect: -0.24, sequence position: 553, from/to: A/G\n",
      "AGTAA * A/G * ACATC\n",
      "\n",
      "Mutation - effect: -0.24, sequence position: 553, from/to: A/C\n",
      "AGTAA * A/C * ACATC\n",
      "\n",
      "Mutation - effect: -0.24, sequence position: 550, from/to: T/A\n",
      "AAAAG * T/A * TAAAC\n",
      "\n",
      "Mutation - effect: -0.24, sequence position: 552, from/to: A/T\n",
      "AAGTA * A/T * AACAT\n",
      "\n",
      "Mutation - effect: 0.24, sequence position: 578, from/to: A/G\n",
      "GTTGT * A/G * ACAAA\n",
      "\n",
      "Mutation - effect: -0.23, sequence position: 551, from/to: A/G\n",
      "AAAGT * A/G * AAACA\n",
      "\n",
      "Mutation - effect: -0.23, sequence position: 549, from/to: G/C\n",
      "AAAAA * G/C * GTAAA\n",
      "\n",
      "Mutation - effect: -0.23, sequence position: 552, from/to: A/G\n",
      "AAGTA * A/G * AACAT\n",
      "\n",
      "Mutation - effect: -0.23, sequence position: 550, from/to: T/G\n",
      "AAAAG * T/G * TAAAC\n",
      "\n",
      "Mutation - effect: -0.23, sequence position: 551, from/to: A/T\n",
      "AAAGT * A/T * AAACA\n",
      "\n",
      "Mutation - effect: -0.22, sequence position: 579, from/to: C/G\n",
      "TTGTA * C/G * CAAAC\n",
      "\n",
      "Mutation - effect: -0.22, sequence position: 580, from/to: A/C\n",
      "TGTAC * A/C * AAACA\n",
      "\n",
      "Mutation - effect: -0.21, sequence position: 554, from/to: C/G\n",
      "GTAAA * C/G * CATCA\n",
      "\n",
      "Mutation - effect: -0.20, sequence position: 580, from/to: A/T\n",
      "TGTAC * A/T * AAACA\n",
      "\n",
      "Mutation - effect: -0.20, sequence position: 582, from/to: A/G\n",
      "TACAA * A/G * ACACA\n",
      "\n",
      "Mutation - effect: -0.20, sequence position: 555, from/to: A/C\n",
      "TAAAC * A/C * ATCAT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import INT_TO_BASES\n",
    "from utils import one_hot_decode\n",
    "\n",
    "top_n_muts, context_size = 5, 5\n",
    "for i in range(20):\n",
    "    mut_eff = ex_mut_effects[i]\n",
    "    seq_pos, nt_idx = mut_eff.seq_idx\n",
    "    decoded_seq = one_hot_decode(seq)\n",
    "    print(\n",
    "        \"Mutation - effect: %.2f, sequence position: %d, from/to: %s/%s\" % \n",
    "        (mut_eff.eff, seq_pos, decoded_seq[seq_pos], INT_TO_BASES[nt_idx])\n",
    "    )\n",
    "    start, end = max(mut_eff.seq_idx[0] - context_size, 0), min(mut_eff.seq_idx[0] + context_size, seq.shape[1])\n",
    "    print(\n",
    "        \"%s * %s/%s * %s\" %\n",
    "        (decoded_seq[start:seq_pos], decoded_seq[seq_pos], INT_TO_BASES[nt_idx], decoded_seq[seq_pos:end])\n",
    "    )\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'logomaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-04c78e04f58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlogomaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'logomaker'"
     ]
    }
   ],
   "source": [
    "from logomaker import Logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
