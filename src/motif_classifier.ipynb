{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# !pip install kipoi\n",
    "# !pip install kipoiseq\n",
    "# !pip install pybedtools\n",
    "# !pip uninstall -y kipoi_veff\n",
    "# !pip install git+https://github.com/an1lam/kipoi-veff\n",
    "# !pip install pyvcf\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import kipoi\n",
    "from kipoi_interpret.importance_scores.ism import Mutation\n",
    "from kipoiseq.dataloaders import SeqIntervalDl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from motif_classifier import MotifClassifier\n",
    "from utils import load_pfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stephenmalina/project/src\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../dat'\n",
    "\n",
    "CHROM_ACC_COL = 'HepG2_DNase_None'\n",
    "TF_COL = 'HepG2_FOXA1_None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DNA sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = SeqIntervalDl(\"../dat/FOXA1.train.samples.bed\", \"../dat/hg19.fa\", auto_resize_len=1000)\n",
    "test_dl = SeqIntervalDl(\"../dat/FOXA1.test.samples.bed\", \"../dat/hg19.fa\", auto_resize_len=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DeepSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "from torch import nn\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsea = kipoi.get_model(\"DeepSEA/predict\", source=\"kipoi\")\n",
    "deepsea.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deepsea.pipeline.predict_example().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs = load_pfm(os.path.join(DATA_DIR, 'foxa1.pfm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and in-silico mutagenesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_dl.load_all()\n",
    "train_seqs = train_data['inputs'].transpose((0, 2, 1)).astype(np.float32)\n",
    "train_labels = train_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_classifier = MotifClassifier(motifs)\n",
    "motif_classifier.train(train_seqs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(motif_classifier.pwm_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_dl.load_all()\n",
    "test_seqs = test_data['inputs'].transpose((0, 2, 1)).astype(np.float32)\n",
    "test_labels = test_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "print(len(test_seqs))\n",
    "scores_preds = [motif_classifier(seq) for seq in test_seqs]\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, [sp[0] for sp in scores_preds])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(fpr, tpr, thresholds)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = sorted([(i, label)\n",
    "                        for i, label in enumerate(deepsea.schema.targets.column_labels)\n",
    "                        if label in [CHROM_ACC_COL, TF_COL]])\n",
    "\n",
    "def output_sel_fn(result):\n",
    "    return np.array([result[:, col_idx] for col_idx, _ in relevant_cols]).T\n",
    "\n",
    "relevant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_seq(it):\n",
    "    return (np\n",
    "            .expand_dims(next(it)[\"inputs\"].transpose(0, 2, 1), 2)\n",
    "            .astype(np.float32)\n",
    "            .squeeze())\n",
    "    \n",
    "\n",
    "epochs, n_seqs, batch_size = 50, 0, 301\n",
    "preds = [[[] for _ in range(n_seqs)] for _ in range(epochs)]\n",
    "it = dl.batch_iter(batch_size=1, num_workers=0, drop_last=False)\n",
    "\n",
    "print(f\"Generating predictions for {len(it)} seqs\")\n",
    "\n",
    "batch_size = 301\n",
    "for i in range(min(n_seqs, len(it))):\n",
    "    seq = next_seq(it)\n",
    "    if np.allclose(seq, .25): continue\n",
    "    wt_mut_batches = generate_wt_mut_batches(seq, batch_size)\n",
    "    for batch in tqdm(wt_mut_batches):\n",
    "        for epoch in range(epochs):\n",
    "            preds[epoch][i].append(output_sel_fn(deepsea.predict_on_batch(np.expand_dims(batch, axis=2))))\n",
    "\n",
    "\n",
    "# np_preds = np.array(preds)\n",
    "# assert np_preds.shape[:2] == (epochs, n_seqs), np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(\n",
    "#     [np.sqrt(epoch_count_vars[:, :, 1:, 1].reshape(-1)) for epoch_count_vars in diff_epoch_count_vars],\n",
    "#     range=[0.0, .3],\n",
    "#     histtype=\"bar\",\n",
    "#     label=[1, 10, 50, 100, 150]\n",
    "# )\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle_file = \"../dat/most_recent_sat_mut_results__drop_channel.pickle\"\n",
    "pickle_file = \"../dat/most_recent_sat_mut_results__original_mc_dropout.pickle\"\n",
    "# with open(pickle_file, 'wb') as f: pickle.dump(np_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_file, 'rb') as f: np_preds = pickle.load(f)\n",
    "print(np_preds.shape)\n",
    "epochs, n_seqs, n_batches, batch_size, _ = np_preds.shape\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_uniform_prob = math.log(.05/(1-.05))\n",
    "def compute_normalized_prob(prob, train_prob):\n",
    "    # source: http://deepsea.princeton.edu/help/\n",
    "    denom = 1+np.exp(-(np.log(prob/(1-prob))+log_uniform_prob-np.log(train_prob/(1-train_prob))))\n",
    "    return 1 / denom\n",
    "\n",
    "# Ratios and normalization formula drawn from here: http://deepsea.princeton.edu/media/help/posproportion.txt\n",
    "tf_compute_normalized_prob = lambda prob: compute_normalized_prob(prob, 0.02394)\n",
    "chrom_acc_normalized_prob = lambda prob: compute_normalized_prob(prob, 0.049791)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Analysis\n",
    "## Computing relevant statistics\n",
    "In this section, we compute the predictive mean and variance of the raw predictions and of each ref/mut pair, which also requires computing covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_preds[:, :, :, :, 0] = chrom_acc_normalized_prob(np_preds[:, :, :, :, 0])\n",
    "np_preds[:, :, :, :, 1] = compute_normalized_prob(np_preds[:, :, :, :, 1], 0.020508)\n",
    "np_preds[:, :, :, :, 2] = compute_normalized_prob(np_preds[:, :, :, :, 2], 0.02394)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = np_preds.shape[2]\n",
    "batch_size = np_preds.shape[3]\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_means = np.mean(np_preds[:, :, :, :, :], axis=0)\n",
    "np_pred_vars = np.var(np_preds, axis=0, dtype=np.float64)\n",
    "np_pred_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_mean_diffs = np_pred_means[:, :, 1:, :] - np_pred_means[:, :, 0:1, :] \n",
    "np_pred_mean_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_covs = np.zeros((n_seqs, n_batches, batch_size, 2, 2, len(relevant_cols)))\n",
    "for seq in range(n_seqs):\n",
    "    for batch in range(n_batches):\n",
    "        for col in range(len(relevant_cols)):\n",
    "            ref_seq_preds = np_preds[:, seq, batch, 0, col]\n",
    "            for mut in range(batch_size):\n",
    "                mut_seq_preds = np_preds[:, seq, batch, mut, col]\n",
    "                cov = np.cov(np.stack((ref_seq_preds, mut_seq_preds)), ddof=0) # 2x2, symmetric\n",
    "                np_pred_covs[seq, batch, mut, :, :, col] = cov # off diag idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_pred_covs[0, 0, 50, :, :, 0])\n",
    "print(np_pred_vars[0, 0, 48:52, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_pred_covs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_uncertainties = np.sqrt(np_pred_covs[:, :, 1:, 1, 1, :] + np_pred_covs[:, :, 1:, 0, 0, :] - 2 * np_pred_covs[:, :, 1:, 0, 1, :])\n",
    "np.mean(np.mean(np_pred_uncertainties, axis=2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_uncertainties = np.sqrt(np_pred_vars[:, :, 1:, :] + np_pred_vars[:, :, 0:1, :] - 2 * np_pred_covs[:, :, 1:, 0, 1, :])\n",
    "np.mean(np.mean(np_pred_uncertainties, axis=2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from prior runs:\n",
    "\n",
    "    array([[0.02932001, 0.16316762, 0.16552047],\n",
    "       [0.15966936, 0.22315478, 0.22175914]])\n",
    "\n",
    "    array([[0.00388914, 0.03407012, 0.03650351],\n",
    "       [0.04776923, 0.09040056, 0.08849534]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy & Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import auc, brier_score_loss, roc_auc_score, roc_curve\n",
    "from sklearn.calibration import calibration_curve \n",
    "\n",
    "score = roc_auc_score(\n",
    "    np.concatenate((np.ones(25), np.zeros(25))),\n",
    "    np_pred_means[:, 0, 0, 1],\n",
    ")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "score = roc_auc_score(\n",
    "    np.concatenate((np.ones(25), np.zeros(25))),\n",
    "    np_pred_means[:, 0, 0, 1],\n",
    ")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = roc_auc_score(\n",
    "    np.concatenate((np.ones(25), np.zeros(25))),\n",
    "    np_pred_means[:, 0, 0, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(\n",
    "    np.concatenate((np.ones(25), np.zeros(25))),\n",
    "    np_pred_means[:, 0, 0, 1],\n",
    ")\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (TF binding)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate((np.ones(25), np.zeros(25)))\n",
    "prob_pos = np_pred_means[:, 0, 0, 1]\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_test,\n",
    "    prob_pos,\n",
    "    n_bins=10\n",
    ")\n",
    "clf_score = brier_score_loss(y_test, prob_pos)\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "     label=\"%s (%1.3f)\" % (\"MC dropout predictive means (TF)\", clf_score))\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "# ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "#          histtype=\"step\", lw=2)\n",
    "prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob_pos, range=(0, 1), bins=10, label=\"Predictive means (TF)\",\n",
    "          histtype=\"step\", lw=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate((np.ones(25), np.zeros(25)))\n",
    "prob_pos = np_pred_means[:, 0, 0, 0]\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_test,\n",
    "    prob_pos,\n",
    "    n_bins=10\n",
    ")\n",
    "clf_score = brier_score_loss(y_test, prob_pos)\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "     label=\"%s (%1.3f)\" % (\"MC dropout predictive means (CA)\", clf_score))\n",
    "plt.ylabel(\"Fraction of positives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 20))\n",
    "plt.suptitle(\"Predictive Means (Ref)\")\n",
    "\n",
    "ax1.hist(\n",
    "    (np_pred_means[:25, 0, 0, 1].reshape(-1),\n",
    "     np_pred_means[25:, 0, 0, 1].reshape(-1)),\n",
    "    label=(\"binding\", \"no binding\"))\n",
    "ax1.set_title(f\"TF {relevant_cols[1][1]}\")\n",
    "ax1.legend()\n",
    "ax2.hist(\n",
    "    (np_pred_means[:25, 0, 0, 0].reshape(-1),\n",
    "     np_pred_means[25:, 0, 0, 0].reshape(-1)),\n",
    "    label=(\"accessible\", \"not accessible\"))\n",
    "ax2.set_title(f\"DNase {relevant_cols[0][1]}\")\n",
    "ax2.legend()\n",
    "\n",
    "fig.text(0.5, 0.08, \"Predictive Means for y=0 vs y=1\", ha=\"center\")\n",
    "fig.text(0.07, 0.5, \"# of Seqs\", va='center', rotation='vertical')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "sample_seq = 2\n",
    "sample_pred_diffs = (np_preds[:, sample_seq, :, 1:, :] - np_preds[:, sample_seq, :, 0:1, :]).reshape(epochs, -1)\n",
    "sample_std_errs = np_pred_uncertainties[sample_seq, :, :, :].reshape(-1)\n",
    "sample_mean_diffs = np_pred_mean_diffs[sample_seq, :, :, :].reshape(-1)\n",
    "normalized_preds = (sample_pred_diffs - sample_mean_diffs) / sample_std_errs\n",
    "\n",
    "res = stats.probplot(normalized_preds[:, :].reshape(-1), plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-CA Relationship and Mutation Effect Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 10))\n",
    "plt.suptitle(\"Predictive Mean Diffs (Mut vs. Ref)\")\n",
    "\n",
    "ax1.hist(\n",
    "    (np_pred_mean_diffs[:25, :, 1:, 1].reshape(-1),\n",
    "     np_pred_mean_diffs[25:, :, 1:, 1].reshape(-1)),\n",
    "    label=(\"binding\", \"no binding\"),\n",
    "    log=True)\n",
    "ax1.set_title(f\"TF {relevant_cols[1][1]}\")\n",
    "ax1.legend()\n",
    "ax2.hist(\n",
    "    (np_pred_mean_diffs[:25, :, 1:, 0].reshape(-1),\n",
    "     np_pred_mean_diffs[25:, :, 1:, 0].reshape(-1)),\n",
    "    label=(\"binding\", \"no binding\"),\n",
    "    log=True)\n",
    "ax2.set_title(f\"DNase {relevant_cols[2][1]}\")\n",
    "ax2.legend()\n",
    "\n",
    "fig.text(0.5, 0.08, \"Predictive Mean Diff\", ha=\"center\")\n",
    "fig.text(0.07, 0.5, \"# of Seqs\", va='center', rotation='vertical')\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_COL = 1\n",
    "CA_COL = 0\n",
    "\n",
    "_, ((ax11, ax12), (ax21, ax22)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "plt.suptitle(\"S.E. vs. Predictive Mean Diff (TF)\")\n",
    "\n",
    "for seq in range(n_seqs):\n",
    "    ax11.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, TF_COL].reshape(-1), \n",
    "        np_pred_uncertainties[seq, :, :, TF_COL].reshape(-1))\n",
    "    ax11.set_title(f\"{relevant_cols[1][1]} (binding)\")\n",
    "\n",
    "for seq in range(n_seqs // 2, n_seqs):\n",
    "    ax12.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, TF_COL].reshape(-1), \n",
    "        np_pred_uncertainties[seq, :, :, TF_COL].reshape(-1))\n",
    "    ax12.set_title(f\"{relevant_cols[1][1]} (no binding)\")\n",
    "    \n",
    "for seq in range(n_seqs // 2):\n",
    "    ax21.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, CA_COL].reshape(-1), \n",
    "        np_pred_uncertainties[seq, :, :, CA_COL].reshape(-1))\n",
    "    ax21.set_title(f\"{relevant_cols[0][1]} (binding)\")\n",
    "\n",
    "for seq in range(n_seqs // 2, n_seqs):\n",
    "    ax22.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, CA_COL].reshape(-1), \n",
    "        np_pred_uncertainties[seq, :, :, CA_COL].reshape(-1))\n",
    "    ax22.set_title(f\"{relevant_cols[0][1]} (no binding)\")\n",
    "    \n",
    "fig.text(0.5, 0.08, \"Predictive Mean Diff\", ha=\"center\")\n",
    "fig.text(0.07, 0.5, \"Predictive S.E.\", va='center', rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_binding_seqs = 25\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 10))\n",
    "plt.suptitle(\"CA predictive mean diff vs. TF predictive mean diff\")\n",
    "for seq in range(n_binding_seqs):\n",
    "    title, ax = (\"(binding)\", ax1) if seq < n_binding_seqs else (\"(no binding)\", ax1)\n",
    "    ax1.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, TF_COL].reshape(-1), \n",
    "        np_pred_mean_diffs[seq, :, :, CA_COL].reshape(-1), \n",
    "        label=seq)\n",
    "    if seq < n_binding_seqs: ax1.set_title(\"(binding)\")\n",
    "    ax1.legend()\n",
    "for seq in range(n_binding_seqs, n_seqs):\n",
    "    ax2.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, TF_COL].reshape(-1), \n",
    "        np_pred_mean_diffs[seq, :, :, CA_COL].reshape(-1), \n",
    "        label=seq)\n",
    "    ax2.set_title(\"(no binding)\")\n",
    "    ax2.legend()\n",
    "fig.text(0.5, 0.08, \"TF Predictive Mean Diff\", ha=\"center\")\n",
    "fig.text(0.07, 0.5, \"CA Predictive Mean Diff\", va='center', rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "from matplotlib import cm\n",
    "\n",
    "cols, margin = 3, 10 # margin determined empirically\n",
    "fig, axs = plt.subplots(math.ceil(n_seqs / float(cols)), cols, figsize=(16, n_seqs + margin))\n",
    "# sample_seqs = np.random.random_integers(0, n_seqs, size=4)\n",
    "# sample_batches = np.random.random_integers(0, n_batches, size=4)\n",
    "# sample_muts = np.random.random_integers(0, batch_size-1, size=4)\n",
    "# sample_col = np.random.random_integers(0, len(relevant_cols)-1)\n",
    "\n",
    "for i in range(n_seqs):\n",
    "    np_sample_mut_preds = np_preds[:, i, :, 1:, TF_COL]\n",
    "    np_sample_ref_preds = np.zeros_like(np_sample_mut_preds) + np_preds[:, i, :, :1, TF_COL]\n",
    "    assert np.allclose(np_sample_ref_preds[:, 0, 0], np_sample_ref_preds[:, 0, 1]) # spot check\n",
    "    colors = (\n",
    "        (np_sample_mut_preds.ravel() - np_sample_mut_preds.mean())**2 + \n",
    "        (np_sample_ref_preds.ravel() - np_sample_ref_preds.mean())**2\n",
    "    )\n",
    "    ax = axs[i // cols, i % cols]\n",
    "    ax.hexbin(\n",
    "        np_sample_ref_preds.ravel(), \n",
    "        np_sample_mut_preds.ravel(), \n",
    "        C=colors,\n",
    "        cmap=cm.jet,\n",
    "        bins=None,\n",
    "    )\n",
    "    xq1, xq2 = np.quantile(np_sample_ref_preds, (.25, .75))\n",
    "    yq1, yq2 = np.quantile(np_sample_mut_preds, (.25, .75))\n",
    "    rect = patches.Rectangle((xq1, yq1), xq2 - xq1, yq2 - yq1, fill=False, edgecolor='black')\n",
    "    rect = ax.add_patch(rect)\n",
    "    xlabel = \"seq {i} ref (std: {stddev:.3f})\".format(i=i, stddev=np.std(np_sample_ref_preds))\n",
    "    ylabel = \"seq {i} mut (std: {stddev:.3f})\".format(i=i, stddev=np.std(np_sample_mut_preds))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "\n",
    "\n",
    "cols, margin = 3, 20 # margin determined empirically\n",
    "fig, axs = plt.subplots(math.ceil(n_seqs / float(cols)), cols, figsize=(16, n_seqs + margin))\n",
    "# plt.suptitle(\"CA predictive mean diff vs. TF predictive mean diff\")\n",
    "slopes = []\n",
    "rsquareds = []\n",
    "\n",
    "for seq in range(n_seqs):\n",
    "    ax = axs[seq // cols, seq % cols]\n",
    "    \n",
    "    x, y = np_pred_mean_diffs[seq, :, :, TF_COL].reshape(-1), np_pred_mean_diffs[seq, :, :, CA_COL].reshape(-1)\n",
    "    xc = sm.add_constant(x)\n",
    "    model = sm.OLS(y, xc)\n",
    "    result = model.fit()\n",
    "    intercept, slope = result.params\n",
    "    slopes.append(slope)\n",
    "    rsquared = result.rsquared\n",
    "    rsquareds.append(rsquared)\n",
    "    stderr = result.bse[1]\n",
    "    \n",
    "    title = \"%d - \" % (seq)\n",
    "    title += \" (b) \" if seq < n_binding_seqs else \" (nb)\"\n",
    "    title += \"(slope: %.2f, r^2: %.3f, std: %.4f)\" % (slope, rsquared, stderr)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    line = slope*x+intercept\n",
    "    prstd, iv_l, iv_u = wls_prediction_std(result)\n",
    "    ax.plot(x, line, 'r')\n",
    "    ax.plot(x, y, 'o')\n",
    "    ax.plot(x, iv_u, 'r--')\n",
    "    ax.plot(x, iv_l, 'r--')\n",
    "    legend = ax.legend(loc=\"best\")\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(slopes) / len(slopes))\n",
    "print(sum(rsquareds) / len(rsquareds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IDX_TO_NT = 'ACGT'\n",
    "\n",
    "def _convert_to_mutation(pos_nt_pair):\n",
    "    return \"%d%s\" % (pos_nt_pair[0], IDX_TO_NT[pos_nt_pair[1]])\n",
    "\n",
    "\n",
    "TF_COL = 1\n",
    "CA_COL = 0\n",
    "\n",
    "def _write_row(writer, seq, batch, i):\n",
    "    seq_num = seq+1\n",
    "    mut_num = (batch * (batch_size)) + i\n",
    "    mut = _convert_to_mutation((mut_num // 3, mut_num % 3))\n",
    "    x_pred_mean_diff = np_pred_mean_diffs[seq, batch, i, TF_COL]\n",
    "    x_pred_uncertainty = np_pred_uncertainties[seq, batch, i, TF_COL]\n",
    "    y_pred_mean_diff = np_pred_mean_diffs[seq, batch, i, CA_COL]\n",
    "    y_pred_uncertainty = np_pred_uncertainties[seq, batch, i, CA_COL]\n",
    "    writer.writerow(\n",
    "        {\n",
    "            \"seq_num\": seq_num,\n",
    "            \"mut\": mut,\n",
    "            \"X_pred_mean\": x_pred_mean_diff,\n",
    "            \"X_pred_var\": x_pred_uncertainty,\n",
    "            \"Y_pred_mean\": y_pred_mean_diff,\n",
    "            \"Y_pred_var\": y_pred_uncertainty,\n",
    "        }\n",
    "    )\n",
    "        \n",
    "\n",
    "with open(\"../dat/means_and_uncertainties_hopefully_correct.csv\", 'w', newline=\"\") as out_file:\n",
    "    fieldnames = [\n",
    "        \"seq_num\",\n",
    "        \"mut\",\n",
    "        \"X_pred_mean\",\n",
    "        \"X_pred_var\",\n",
    "        \"Y_pred_mean\",\n",
    "        \"Y_pred_var\",\n",
    "    ]\n",
    "    writer = csv.DictWriter(out_file, delimiter=\",\", fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for seq in range(n_seqs):\n",
    "        _write_row(writer, seq, batch, i)\n",
    "        for batch in range(n_batches):\n",
    "            for i in range(0, batch_size-1):\n",
    "                _write_row(writer, seq, batch, i)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros = np.zeros((2, 3))\n",
    "# def sanitize_scores(scores):\n",
    "#     orig_shape = scores.shape\n",
    "#     sanitized_scores = np.ndarray((*orig_shape, 2, 3), dtype=scores.dtype)\n",
    "#     flattened_scores = scores.reshape(-1)\n",
    "    \n",
    "#     for i, score in enumerate(flattened_scores):\n",
    "#         idx = np.unravel_index(i, orig_shape)\n",
    "#         if score is None: sanitized_scores[idx] = zeros\n",
    "#         else: sanitized_scores[idx] = np.array(score)\n",
    "#     return sanitized_scores\n",
    "\n",
    "# sanitized_scores = sanitize_scores(np.squeeze(ism_score))\n",
    "# ctcf_original_preds = sanitized_scores[:, :, :, 0, 1]\n",
    "# ctcf_pred_diffs = sanitized_scores[:, :, :, 1, 1]\n",
    "# dnase_original_preds = sanitized_scores[:, :, :, 0, 0]\n",
    "# dnase_pred_diff = sanitized_scores[:, :, :, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_uniform_prop = math.log(.05/(1-.05))\n",
    "def compute_normalized_prob(prob, train_prob):\n",
    "    denom = 1+np.exp(-(np.log(prob/(1-prob))+log_uniform_prop-np.log(train_prob/(1-train_prob))))\n",
    "    return 1 / denom\n",
    "\n",
    "# Ratios and normalization formula drawn from here: http://deepsea.princeton.edu/media/help/posproportion.txt\n",
    "# tf_compute_normalized_prob = lambda prob: compute_normalized_prob(prob, .020029)\n",
    "# ENCODE\tA549\tDNase\tNone\t0.048136\n",
    "# chrom_acc_normalized_prob = lambda prob: compute_normalized_prob(prob, 0.048136)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
