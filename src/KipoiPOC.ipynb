{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kipoi\n",
    "# !pip install kipoiseq\n",
    "# !pip install pybedtools\n",
    "# !pip uninstall -y kipoi_veff\n",
    "# !pip install git+https://github.com/an1lam/kipoi-veff\n",
    "# !pip install pyvcf\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import kipoi\n",
    "from kipoi_interpret.importance_scores.ism import Mutation\n",
    "from kipoiseq.dataloaders import SeqIntervalDl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stephenmalina/project/src\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DNA sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "dl = SeqIntervalDl(\"../dat/50_random_seqs.bed\", \"../dat/hg19.fa\", auto_resize_len=1000)\n",
    "data = dl.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 4, 1, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = np.expand_dims(data['inputs'].transpose(0, 2, 1), 2).astype(np.float32)\n",
    "seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DeepSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.14.0\n",
      "torch version: 1.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = kipoi.list_models()\n",
    "# deepsea_models = df[df.model.str.contains(\"DeepSEA\")]\n",
    "# deepsea_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/stephenmalina/.kipoi/models/DeepSEA/predict/downloaded/model_files/weights/89e640bf6bdbe1ff165f484d9796efc7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ReCodeAlphabet()\n",
       "  (1): ConcatenateRC()\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(4, 320, kernel_size=(1, 8), stride=(1, 1))\n",
       "    (1): Threshold(threshold=0, value=1e-06)\n",
       "    (2): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Conv2d(320, 480, kernel_size=(1, 8), stride=(1, 1))\n",
       "    (5): Threshold(threshold=0, value=1e-06)\n",
       "    (6): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv2d(480, 960, kernel_size=(1, 8), stride=(1, 1))\n",
       "    (9): Threshold(threshold=0, value=1e-06)\n",
       "    (10): Dropout(p=0.5, inplace=False)\n",
       "    (11): Lambda()\n",
       "    (12): Sequential(\n",
       "      (0): Lambda()\n",
       "      (1): Linear(in_features=50880, out_features=925, bias=True)\n",
       "    )\n",
       "    (13): Threshold(threshold=0, value=1e-06)\n",
       "    (14): Sequential(\n",
       "      (0): Lambda()\n",
       "      (1): Linear(in_features=925, out_features=919, bias=True)\n",
       "    )\n",
       "    (15): Sigmoid()\n",
       "  )\n",
       "  (3): AverageRC()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsea = kipoi.get_model(\"DeepSEA/predict\", source=\"kipoi\")\n",
    "deepsea.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 919)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsea.pipeline.predict_example().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacking DeepSEA's layers\n",
    "This section is focused on tweaking DeepSEA's model to use dropout when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Legally) grabbed from:\n",
    "# https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/lock_dropout.html.\n",
    "class LockedDropout(nn.Module):\n",
    "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
    "\n",
    "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. \n",
    "    Here is their `License\n",
    "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
    "\n",
    "    Args:\n",
    "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5, training=True):\n",
    "        self.p = p\n",
    "        self.training = training\n",
    "        super().__init__()\n",
    "\n",
    "    def train(self, training=True):\n",
    "        self.training = training\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]):\n",
    "                Input to apply dropout to.\n",
    "        \"\"\"\n",
    "        if not self.training or not self.p:\n",
    "            return x\n",
    "        x = x.clone()\n",
    "        mask = x.new_empty(1, x.size(1), x.size(2), x.size(3), requires_grad=False)\n",
    "        mask = mask.bernoulli_(1 - self.p)\n",
    "        mask = mask.div_(1 - self.p) # rescaling\n",
    "        mask = mask.expand_as(x)\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + str(self.p) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dropout_layers(model):\n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            # recurse\n",
    "            model._modules[name] = replace_dropout_layers(module)\n",
    "\n",
    "        if type(module) == nn.Dropout:\n",
    "             model._modules[name] = LockedDropout(module.p, training=module.training)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dropout(m):\n",
    "    if type(m) == nn.Dropout or type(m) == LockedDropout:\n",
    "        m.train()\n",
    "        \n",
    "def unapply_dropout(m):\n",
    "    if type(m) == nn.Dropout or type(m) == LockedDropout:\n",
    "        m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsea.model = replace_dropout_layers(deepsea.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying dropout is being applied\n",
    "We can verify or at least get reasonable certainty that dropout's being applied by running 10 predictions on the same sequence and then seeing that the predicted binding probs differ. First, we show that the predictions stay the same if dropout is off.\n",
    "\n",
    "Then, we turn on dropout and show that they start to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 33.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few preds (1/3):  [[0.08161031 0.06867661 0.10076798 ... 0.09493414 0.02133885 0.01201447]\n",
      " [0.06698208 0.01062424 0.02694638 ... 0.15490864 0.04822354 0.00770117]\n",
      " [0.04445557 0.00539728 0.018408   ... 0.14994667 0.35297143 0.02272817]\n",
      " [0.00245111 0.00048716 0.00247776 ... 0.01495204 0.04857798 0.00068955]\n",
      " [0.00076919 0.00535259 0.00189638 ... 0.01889143 0.06013346 0.02190239]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 33.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few preds (2/3):  [[0.08161031 0.06867661 0.10076798 ... 0.09493414 0.02133885 0.01201447]\n",
      " [0.06698208 0.01062424 0.02694638 ... 0.15490864 0.04822354 0.00770117]\n",
      " [0.04445557 0.00539728 0.018408   ... 0.14994667 0.35297143 0.02272817]\n",
      " [0.00245111 0.00048716 0.00247776 ... 0.01495204 0.04857798 0.00068955]\n",
      " [0.00076919 0.00535259 0.00189638 ... 0.01889143 0.06013346 0.02190239]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 34.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few preds (3/3):  [[0.08161031 0.06867661 0.10076798 ... 0.09493414 0.02133885 0.01201447]\n",
      " [0.06698208 0.01062424 0.02694638 ... 0.15490864 0.04822354 0.00770117]\n",
      " [0.04445557 0.00539728 0.018408   ... 0.14994667 0.35297143 0.02272817]\n",
      " [0.00245111 0.00048716 0.00247776 ... 0.01495204 0.04857798 0.00068955]\n",
      " [0.00076919 0.00535259 0.00189638 ... 0.01889143 0.06013346 0.02190239]]\n"
     ]
    }
   ],
   "source": [
    "deepsea.model = deepsea.model.apply(unapply_dropout)\n",
    "for i in range(3):\n",
    "    print(f\"First few preds ({i+1}/3): \", deepsea.pipeline.predict_example()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 33.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few preds (1/3):  [[0.09469041 0.06476855 0.12214313 ... 0.09340209 0.02927886 0.00951671]\n",
      " [0.05391673 0.00865934 0.02343167 ... 0.10413048 0.04671773 0.0094504 ]\n",
      " [0.04761756 0.00292272 0.02001821 ... 0.10483676 0.3426831  0.02824711]\n",
      " [0.00287849 0.00051717 0.00311738 ... 0.00953412 0.04629721 0.00129358]\n",
      " [0.00076278 0.0087555  0.00196435 ... 0.01482    0.05284825 0.0233275 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few preds (2/3):  [[0.07072315 0.09313697 0.10903168 ... 0.08075188 0.01988692 0.01275267]\n",
      " [0.0755543  0.02080756 0.03763523 ... 0.18903339 0.04289904 0.00842006]\n",
      " [0.04845877 0.00574616 0.02143795 ... 0.09896583 0.2500741  0.02560952]\n",
      " [0.00172383 0.00054227 0.00215096 ... 0.00909489 0.02452013 0.00052947]\n",
      " [0.00071932 0.00820777 0.00236618 ... 0.01169196 0.05650319 0.01962058]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 33.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few preds (3/3):  [[0.09125951 0.03321379 0.10498534 ... 0.05581583 0.02147745 0.02208168]\n",
      " [0.04562641 0.00490076 0.02036985 ... 0.19676986 0.05821128 0.00898473]\n",
      " [0.05367801 0.00396026 0.02989383 ... 0.04869422 0.19736491 0.03130752]\n",
      " [0.00518991 0.00090631 0.00659967 ... 0.01447539 0.05181357 0.00093238]\n",
      " [0.00074766 0.00480332 0.00170046 ... 0.02407834 0.05369303 0.03029319]]\n"
     ]
    }
   ],
   "source": [
    "deepsea.model.eval()\n",
    "deepsea.model = deepsea.model.apply(apply_dropout)\n",
    "for i in range(3):\n",
    "    print(f\"First few preds ({i+1}/3): \", deepsea.pipeline.predict_example()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and in-silico mutagenesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00947323, 0.02589644, 0.00898295, ..., 0.02471472, 0.09217416,\n",
       "        0.0264926 ],\n",
       "       [0.00121323, 0.00887009, 0.00260588, ..., 0.02647636, 0.07006134,\n",
       "        0.00369652],\n",
       "       [0.00248037, 0.00347161, 0.00266787, ..., 0.01151872, 0.05654726,\n",
       "        0.01730141],\n",
       "       [0.00351746, 0.00387447, 0.00372575, ..., 0.00689738, 0.04173702,\n",
       "        0.00717506],\n",
       "       [0.00085676, 0.00292669, 0.00347074, ..., 0.02656512, 0.26414126,\n",
       "        0.00315961]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsea.predict_on_batch(seqs[:5, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zeros = np.zeros((4,))\n",
    "def generate_wt_mut_batches(seq, batch_size):\n",
    "    \"\"\"\n",
    "    For a given sequence, generate all possible point-mutated versions of the sequence\n",
    "    in batches of size `param:batch_size`.\n",
    "    \n",
    "    Args:\n",
    "        seq (numpy.ndarray [number of base pairs, sequence length]): \n",
    "            wild type sequence.\n",
    "        batch_size (int): size of returned batches. Note that each batch will have the\n",
    "            wild type sequence as its first row since we need to compute wild type / mut\n",
    "            prediction diffs using predictions generated by the same dropout mask.\n",
    "    \"\"\"\n",
    "    num_nts, seq_len = seq.shape\n",
    "    assert ((seq_len * 3) % (batch_size-1)) == 0, seq_len * 3\n",
    "    # 3 mutations per nt and then account for ref in each batch\n",
    "    n_batches = (seq_len * 3) // (batch_size-1)\n",
    "    seq_batch = seq[np.newaxis, :, :].repeat(batch_size, axis=0)\n",
    "    seq_batches = seq_batch[np.newaxis, :, :, :].repeat(n_batches, axis=0)\n",
    "    i = 0\n",
    "    for seq_idx in range(seq_len):  # iterate over sequence \n",
    "        for nt_idx in range(num_nts):  # iterate over nucleotides\n",
    "            curr_batch, curr_idx = i // (batch_size - 1), (i % (batch_size-1) + 1)\n",
    "            \n",
    "            curr_nt = seq[nt_idx, seq_idx]\n",
    "            if int(curr_nt) == 1: continue\n",
    "\n",
    "            seq_batches[curr_batch, curr_idx, :, seq_idx] = all_zeros\n",
    "            seq_batches[curr_batch, curr_idx, nt_idx, seq_idx] = 1\n",
    "            i += 1\n",
    "    return seq_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(56, 'HepG2_DNase_None'), (304, 'HepG2_FOXA2_None')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHROM_ACC_COL = 'HepG2_DNase_None'\n",
    "# TF_COL = 'A549_CTCF_None'\n",
    "TF_COL = 'HepG2_FOXA2_None'\n",
    "relevant_cols = sorted([(i, label)\n",
    "                        for i, label in enumerate(deepsea.schema.targets.column_labels)\n",
    "                        if label in [CHROM_ACC_COL, TF_COL]])\n",
    "\n",
    "def output_sel_fn(result):\n",
    "    return np.array([result[:, col_idx] for col_idx, _ in relevant_cols]).T\n",
    "\n",
    "relevant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21925.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22086.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 16282.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23696.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19328.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21959.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21034.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 20992.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 18800.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21799.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23301.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22156.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21215.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19039.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23458.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 24600.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21377.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22405.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22733.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 20702.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 20490.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19887.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22262.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21959.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22832.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21653.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 24314.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21948.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23340.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22121.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22017.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 20010.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19409.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23172.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 20164.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19517.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19099.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21498.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22758.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19756.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22417.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23392.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 21720.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23250.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 1.0 9\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23314.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 19013.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 999 0.0 9\n",
      "passed cont\n",
      "3000 999 1.0 10\n",
      "3000 1000 10\n",
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23121.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22770.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 23366.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "2999 1000 0.0 9\n",
      "passed cont\n",
      "3000 1000 9\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 22635.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def next_seq(it):\n",
    "    return (np\n",
    "            .expand_dims(next(it)[\"inputs\"].transpose(0, 2, 1), 2)\n",
    "            .astype(np.float32)\n",
    "            .squeeze())\n",
    "    \n",
    "\n",
    "epochs, n_seqs, batch_size = 1, 51, 301\n",
    "preds = [[[] for _ in range(n_seqs)] for _ in range(epochs)]\n",
    "it = dl.batch_iter(batch_size=1, num_workers=0, drop_last=False)\n",
    "print(len(it))\n",
    "batch_size = 301\n",
    "for i in range(min(n_seqs, len(it))):\n",
    "    seq = next_seq(it)\n",
    "    if np.allclose(seq, .25): continue\n",
    "    wt_mut_batches = generate_wt_mut_batches(seq, batch_size)\n",
    "    for batch in tqdm(wt_mut_batches):\n",
    "        for epoch in range(epochs):\n",
    "            preds[epoch][i].append(output_sel_fn(deepsea.predict_on_batch(np.expand_dims(batch, axis=2))))\n",
    "\n",
    "\n",
    "np_preds = np.array(preds)\n",
    "# assert np_preds.shape[:2] == (epochs, n_seqs), np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file = \"../dat/most_recent_sat_mut_results.pickle\"\n",
    "with open(pickle_file, 'wb') as f: pickle.dump(np_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(pickle_file, 'rb') as f: np_preds = pickle.load(f)\n",
    "np_preds.shape\n",
    "epochs, n_seqs, n_batches, batch_size, _ = np_preds.shape\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_log_odds_preds = np.log(np_preds / (1-np_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = np_preds.shape[2]\n",
    "batch_size = np_preds.shape[3]\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_means = np.mean(np_preds[:, :batch_size-1, :, :, :], axis=0)\n",
    "np_pred_vars = np.var(np_preds, axis=0)\n",
    "np_pred_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_mean_diffs = np_pred_means[:, :, 1:, :] - np_pred_means[:, :, 0:1, :] \n",
    "np_pred_mean_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_covs = np.zeros((n_seqs, n_batches, batch_size, len(relevant_cols)))\n",
    "for seq in range(n_seqs):\n",
    "    for batch in range(n_batches):\n",
    "        for col in range(len(relevant_cols)):\n",
    "            ref_seq_preds = np_preds[:, seq, batch, 0, col]\n",
    "            for mut in range(batch_size):\n",
    "                mut_seq_preds = np_preds[:, seq, batch, mut, col]\n",
    "                cov = np.cov(np.stack((ref_seq_preds, mut_seq_preds))) # 2x2, symmetric\n",
    "                np_pred_covs[seq, batch, mut, col] = cov[0, 1] # off diag idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred_uncertainties = np.sqrt(np_pred_vars[:, :, 1:, :] + np_pred_vars[:, :, 0:1, :]) - 2 * np_pred_covs[:, :, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np_pred_mean_diffs[:, :n_batches, 1:, 1].reshape(-1), log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for seq in range(np_pred_means.shape[0]):\n",
    "    plt.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, TF_COL].reshape(-1), \n",
    "        np_pred_uncertainties[seq, :, :, TF_COL].reshape(-1),\n",
    "        label=seq)\n",
    "plt.xlabel(\"Predictive Mean Diff (TF)\")\n",
    "plt.ylabel(\"Predictive Variance (TF)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for seq in range(np_pred_means.shape[0]):\n",
    "    plt.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, CA_COL].reshape(-1), \n",
    "        np_pred_uncertainties[seq, :, :, CA_COL].reshape(-1),\n",
    "        label=seq)\n",
    "plt.xlabel(\"Predictive Mean Diff (CA)\")\n",
    "plt.ylabel(\"Predictive Variance (CA)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_COL = 1\n",
    "CA_COL = 0\n",
    "\n",
    "for seq in range(np_pred_means.shape[0]):\n",
    "    plt.scatter(\n",
    "        (np_pred_means[seq, :, 1:, TF_COL] - np_pred_means[seq, :, 0:1, TF_COL]).reshape(-1), \n",
    "        (np_pred_means[seq, :, 1:, CA_COL] - np_pred_means[seq, :, 0:1, CA_COL]).reshape(-1), \n",
    "        label=seq)\n",
    "plt.xlabel(\"TF pred diff\")\n",
    "plt.ylabel(\"CA pred diff\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for seq in range(np_pred_means.shape[0]):\n",
    "    print(np_pred_mean_diffs.reshape(-1).shape)\n",
    "    print(np_pred_uncertainties.reshape(-1).shape)\n",
    "    plt.scatter(\n",
    "        np_pred_mean_diffs[seq, :, :, TF_COL].reshape(-1), \n",
    "        np_pred_uncertainties[seq, :, :, TF_COL].reshape(-1),\n",
    "        label=seq)\n",
    "plt.xlabel(\"Instrument Strength\")\n",
    "plt.ylabel(\"Predictive Uncertainty\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "sample_seqs = np.random.random_integers(0, n_seqs-1, size=4)\n",
    "sample_batches = np.random.random_integers(0, n_batches-1, size=4)\n",
    "sample_muts = np.random.random_integers(0, batch_size-1, size=4)\n",
    "sample_col = np.random.random_integers(0, len(relevant_cols)-1)\n",
    "\n",
    "for i in range(4):\n",
    "    np_sample_ref_preds = np_preds[\n",
    "        :, sample_seqs[i], sample_batches[i], 0, sample_col\n",
    "    ]\n",
    "    np_sample_mut_preds = np_preds[\n",
    "        :, sample_seqs[i], sample_batches[i], sample_muts[i], sample_col\n",
    "    ]\n",
    "    axs[i // 2, i % 2].scatter(np_sample_ref_preds, np_sample_mut_preds)\n",
    "    axs[i // 2, i % 2].set_xlabel(f\"{relevant_cols[sample_col][1]} prob (ref)\")\n",
    "    axs[i // 2, i % 2].set_ylabel(f\"{relevant_cols[sample_col][1]} prob (mut)\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IDX_TO_NT = 'ACGT'\n",
    "\n",
    "def _convert_to_mutation(pos_nt_pair):\n",
    "    return \"%d%s\" % (pos_nt_pair[0], IDX_TO_NT[pos_nt_pair[1]])\n",
    "\n",
    "\n",
    "TF_COL = 1\n",
    "CA_COL = 0\n",
    "\n",
    "def _write_row(writer, seq, batch, i):\n",
    "    seq_num = seq+1\n",
    "    mut_num = (batch * (batch_size)) + i\n",
    "    mut = _convert_to_mutation((mut_num // 3, mut_num % 3))\n",
    "    x_pred_mean_diff = np_pred_mean_diffs[seq, batch, i, TF_COL]\n",
    "    x_pred_uncertainty = np_pred_uncertainties[seq, batch, i, TF_COL]\n",
    "    y_pred_mean_diff = np_pred_mean_diffs[seq, batch, i, CA_COL]\n",
    "    y_pred_uncertainty = np_pred_uncertainties[seq, batch, i, CA_COL]\n",
    "    writer.writerow(\n",
    "        {\n",
    "            \"seq_num\": seq_num,\n",
    "            \"mut\": mut,\n",
    "            \"X_pred_mean\": x_pred_mean_diff,\n",
    "            \"X_pred_var\": x_pred_uncertainty,\n",
    "            \"Y_pred_mean\": y_pred_mean_diff,\n",
    "            \"Y_pred_var\": y_pred_uncertainty,\n",
    "        }\n",
    "    )\n",
    "        \n",
    "\n",
    "with open(\"../dat/means_and_uncertainties.csv\", 'w', newline=\"\") as out_file:\n",
    "    fieldnames = [\n",
    "        \"seq_num\",\n",
    "        \"mut\",\n",
    "        \"X_pred_mean\",\n",
    "        \"X_pred_var\",\n",
    "        \"Y_pred_mean\",\n",
    "        \"Y_pred_var\",\n",
    "    ]\n",
    "    writer = csv.DictWriter(out_file, delimiter=\",\", fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for seq in range(n_seqs):\n",
    "        _write_row(writer, seq, batch, i)\n",
    "        for batch in range(n_batches):\n",
    "            for i in range(0, batch_size-1):\n",
    "                _write_row(writer, seq, batch, i)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros = np.zeros((2, 3))\n",
    "# def sanitize_scores(scores):\n",
    "#     orig_shape = scores.shape\n",
    "#     sanitized_scores = np.ndarray((*orig_shape, 2, 3), dtype=scores.dtype)\n",
    "#     flattened_scores = scores.reshape(-1)\n",
    "    \n",
    "#     for i, score in enumerate(flattened_scores):\n",
    "#         idx = np.unravel_index(i, orig_shape)\n",
    "#         if score is None: sanitized_scores[idx] = zeros\n",
    "#         else: sanitized_scores[idx] = np.array(score)\n",
    "#     return sanitized_scores\n",
    "\n",
    "# sanitized_scores = sanitize_scores(np.squeeze(ism_score))\n",
    "# ctcf_original_preds = sanitized_scores[:, :, :, 0, 1]\n",
    "# ctcf_pred_diffs = sanitized_scores[:, :, :, 1, 1]\n",
    "# dnase_original_preds = sanitized_scores[:, :, :, 0, 0]\n",
    "# dnase_pred_diff = sanitized_scores[:, :, :, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_uniform_prop = math.log(.05/(1-.05))\n",
    "def compute_normalized_prob(prob, train_prob):\n",
    "    denom = 1+np.exp(-(np.log(prob/(1-prob))+log_uniform_prop-np.log(train_prob/(1-train_prob))))\n",
    "    return 1 / denom\n",
    "\n",
    "# Ratios and normalization formula drawn from here: http://deepsea.princeton.edu/media/help/posproportion.txt\n",
    "# tf_compute_normalized_prob = lambda prob: compute_normalized_prob(prob, .020029)\n",
    "# ENCODE\tA549\tDNase\tNone\t0.048136\n",
    "# chrom_acc_normalized_prob = lambda prob: compute_normalized_prob(prob, 0.048136)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
