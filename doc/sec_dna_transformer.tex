\section{Transformers: DNA Disguised}
\subsection{Relevant Work}
\begin{itemize}

    \item \hyperlink{https://github.com/kheyer/Genomic-ULMFiT}{Genomic ULMFit}: uses an NLP pre-training technique called ULMFit to do SOTA on a bunch of different genomic prediction tasks. Only code, no paper though.
    \item \cite{Rives2019-re} scales Transformer training to 250 million proteins and test their model's ability to classify proteins, predict their alignment features, and more.
    \item \cite{Quang2016-ar} uses a hybrid convolutional/recurrent architecture to predc
\end{itemize}

\subsection{Elevator Pitch}
\begin{itemize}
    \item The ground truth for DNA is the sequence information. 
    \item Transformers seem to work fairly well for learning info about proteins.
    \item Biology people aren't going to buy into these sorts of methods until someone shows that they can actually be used to go deep into a biological problem.
\end{itemize}

\subsection{Risks}
Will Transformers work well on sequences pulled from a 4-character alphabet?

\subsection{Potential Tasks}
\begin{itemize}
    \item Identifying chromatin state, i.e. compare to ChromHMM.
    \item Predicting transcription factor binding.
    \item Replicating DeepCpg.
\end{itemize}

