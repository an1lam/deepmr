@article{burgess2014network,
  title={Network Mendelian randomization: using genetic variants as instrumental variables to investigate mediation in causal pathways},
  author={Burgess, Stephen and Daniel, Rhian M and Butterworth, Adam S and Thompson, Simon G and EPIC-InterAct Consortium},
  journal={International journal of epidemiology},
  volume={44},
  number={2},
  pages={484--495},
  year={2014},
  publisher={Oxford University Press}
}


@misc{zhao2018statistical,
    title={Statistical inference in two-sample summary-data Mendelian randomization using robust adjusted profile score},
    author={Qingyuan Zhao and Jingshu Wang and Gibran Hemani and Jack Bowden and Dylan S. Small},
    year={2018},
    eprint={1801.09652},
    archivePrefix={arXiv},
    primaryClass={stat.AP}
}

@article{alipanahi2015predicting,
  title={Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning},
  author={Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey, Brendan J},
  journal={Nature biotechnology},
  volume={33},
  number={8},
  pages={831},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{zhou2015predicting,
  title={Predicting effects of noncoding variants with deep learning--based sequence model},
  author={Zhou, Jian and Troyanskaya, Olga G},
  journal={Nature methods},
  volume={12},
  number={10},
  pages={931},
  year={2015},
  publisher={Nature Publishing Group}
}



@ARTICLE{Didelez2007-vs,
  title    = "Mendelian randomization as an instrumental variable approach to
              causal inference",
  author   = "Didelez, Vanessa and Sheehan, Nuala",
  abstract = "In epidemiological research, the causal effect of a modifiable
              phenotype or exposure on a disease is often of public health
              interest. Randomized controlled trials to investigate this effect
              are not always possible and inferences based on observational
              data can be confounded. However, if we know of a gene closely
              linked to the phenotype without direct effect on the disease, it
              can often be reasonably assumed that the gene is not itself
              associated with any confounding factors - a phenomenon called
              Mendelian randomization. These properties define an instrumental
              variable and allow estimation of the causal effect, despite the
              confounding, under certain model restrictions. In this paper, we
              present a formal framework for causal inference based on
              Mendelian randomization and suggest using directed acyclic graphs
              to check model assumptions by visual inspection. This framework
              allows us to address limitations of the Mendelian randomization
              technique that have often been overlooked in the medical
              literature.",
  journal  = "Stat. Methods Med. Res.",
  volume   =  16,
  number   =  4,
  pages    = "309--330",
  month    =  aug,
  year     =  2007,
  language = "en"
}


@ARTICLE{Lai2018-fr,
  title    = "Pioneer Factors in Animals and {Plants-Colonizing} Chromatin for
              Gene Regulation",
  author   = "Lai, Xuelei and Verhage, Leonie and Hugouvieux, Veronique and
              Zubieta, Chloe",
  abstract = "Unlike most transcription factors (TF), pioneer TFs have a
              specialized role in binding closed regions of chromatin and
              initiating the subsequent opening of these regions. Thus, pioneer
              TFs are key factors in gene regulation with critical roles in
              developmental transitions, including organ biogenesis, tissue
              development, and cellular differentiation. These developmental
              events involve some major reprogramming of gene expression
              patterns, specifically the opening and closing of distinct
              chromatin regions. Here, we discuss how pioneer TFs are
              identified using biochemical and genome-wide techniques. What is
              known about pioneer TFs from animals and plants is reviewed, with
              a focus on the strategies used by pioneer factors in different
              organisms. Finally, the different molecular mechanisms pioneer
              factors used are discussed, highlighting the roles that tertiary
              and quaternary structures play in nucleosome-compatible
              DNA-binding.",
  journal  = "Molecules",
  volume   =  23,
  number   =  8,
  month    =  jul,
  year     =  2018,
  keywords = "cell fate transition; chromatin accessibility; pioneer activity;
              transcription factor",
  language = "en"
}

@article{kelley2016basset,
  title={Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks},
  author={Kelley, David R and Snoek, Jasper and Rinn, John L},
  journal={Genome research},
  volume={26},
  number={7},
  pages={990--999},
  year={2016},
  publisher={Cold Spring Harbor Lab}
}

@article{st0030,
	author = "Baum, C. F.",
	author = "Schaffer, M. E.",
	author = "Stillman, S.",
	title = "Instrumental variables and GMM: Estimation and testing",
	journal = "Stata Journal",
	publisher = "Stata Press",
	address = "College Station, TX",
	volume = "3",
	number = "1",
	year = "2003",
	pages = "1-31(31)",
	url = "http://www.stata-journal.com/article.html?article=st0030"
}

@article{kelley2018sequential,
  title={Sequential regulatory activity prediction across chromosomes with convolutional neural networks},
  author={Kelley, David R and Reshef, Yakir A and Bileschi, Maxwell and Belanger, David and McLean, Cory Y and Snoek, Jasper},
  journal={Genome research},
  volume={28},
  number={5},
  pages={739--750},
  year={2018},
  publisher={Cold Spring Harbor Lab}
}

@article{zheng2018deep,
  title={Deep-RBPPred: Predicting RNA binding proteins in the proteome scale based on deep learning},
  author={Zheng, Jinfang and Zhang, Xiaoli and Zhao, Xunyi and Tong, Xiaoxue and Hong, Xu and Xie, Juan and Liu, Shiyong},
  journal={Scientific reports},
  volume={8},
  number={1},
  pages={15264},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{zhang2019deepdrbp,
  title={DeepDRBP-2L: a new genome annotation predictor for identifying DNA binding proteins and RNA binding proteins using Convolutional Neural Network and Long Short-Term Memory},
  author={Zhang, Jun and Chen, Qingcai and Liu, Bin},
  journal={IEEE/ACM transactions on computational biology and bioinformatics},
  year={2019},
  publisher={IEEE}
}

@article{koo2018inferring,
  title={Inferring Sequence-Structure Preferences of RNA-Binding Proteins with Convolutional Residual Networks},
  author={Koo, Peter K and Anand, Praveen and Paul, Steffan B and Eddy, Sean R},
  journal={bioRxiv},
  pages={418459},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@article{angermueller2017deepcpg,
  title={DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning},
  author={Angermueller, Christof and Lee, Heather J and Reik, Wolf and Stegle, Oliver},
  journal={Genome biology},
  volume={18},
  number={1},
  pages={67},
  year={2017},
  publisher={BioMed Central}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4765--4774},
  year={2017}
}

@book{pearl2000causality,
  title={Causality: models, reasoning and inference},
  author={Pearl, Judea},
  volume={29},
  year={2000},
  publisher={Springer}
}


@ARTICLE{Gal2015-st,
  title         = "Dropout as a Bayesian Approximation: Representing Model
                   Uncertainty in Deep Learning",
  author        = "Gal, Yarin and Ghahramani, Zoubin",
  abstract      = "Deep learning tools have gained tremendous attention in
                   applied machine learning. However such tools for regression
                   and classification do not capture model uncertainty. In
                   comparison, Bayesian models offer a mathematically grounded
                   framework to reason about model uncertainty, but usually
                   come with a prohibitive computational cost. In this paper we
                   develop a new theoretical framework casting dropout training
                   in deep neural networks (NNs) as approximate Bayesian
                   inference in deep Gaussian processes. A direct result of
                   this theory gives us tools to model uncertainty with dropout
                   NNs -- extracting information from existing models that has
                   been thrown away so far. This mitigates the problem of
                   representing uncertainty in deep learning without
                   sacrificing either computational complexity or test
                   accuracy. We perform an extensive study of the properties of
                   dropout's uncertainty. Various network architectures and
                   non-linearities are assessed on tasks of regression and
                   classification, using MNIST as an example. We show a
                   considerable improvement in predictive log-likelihood and
                   RMSE compared to existing state-of-the-art methods, and
                   finish by using dropout's uncertainty in deep reinforcement
                   learning.",
  month         =  jun,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1506.02142"
}


@INPROCEEDINGS{Hartford2017-cm,
  title     = "Deep {{IV}}: A Flexible Approach for Counterfactual Prediction",
  booktitle = "Proceedings of the 34th International Conference on Machine
               Learning",
  author    = "Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and
               Taddy, Matt",
  editor    = "Precup, Doina and Teh, Yee Whye",
  abstract  = "Counterfactual prediction requires understanding causal
               relationships between so-called treatment and outcome variables.
               This paper provides a recipe for augmenting deep learning
               methods to accurately characterize such relationships in the
               presence of instrument variables (IVs) -- sources of treatment
               randomization that are conditionally independent from the
               outcomes. Our IV specification resolves into two prediction
               tasks that can be solved with deep neural nets: a first-stage
               network for treatment prediction and a second-stage network
               whose loss function involves integration over the conditional
               treatment distribution. This Deep IV framework allows us to take
               advantage of off-the-shelf supervised learning techniques to
               estimate causal effects by adapting the loss function.
               Experiments show that it outperforms existing machine learning
               approaches.",
  publisher = "PMLR",
  volume    =  70,
  pages     = "1414--1423",
  series    = "Proceedings of Machine Learning Research",
  year      =  2017,
  address   = "International Convention Centre, Sydney, Australia"
}


@ARTICLE{Guo2017-qx,
  title         = "On Calibration of Modern Neural Networks",
  author        = "Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger,
                   Kilian Q",
  abstract      = "Confidence calibration -- the problem of predicting
                   probability estimates representative of the true correctness
                   likelihood -- is important for classification models in many
                   applications. We discover that modern neural networks,
                   unlike those from a decade ago, are poorly calibrated.
                   Through extensive experiments, we observe that depth, width,
                   weight decay, and Batch Normalization are important factors
                   influencing calibration. We evaluate the performance of
                   various post-processing calibration methods on
                   state-of-the-art architectures with image and document
                   classification datasets. Our analysis and experiments not
                   only offer insights into neural network learning, but also
                   provide a simple and straightforward recipe for practical
                   settings: on most datasets, temperature scaling -- a
                   single-parameter variant of Platt Scaling -- is surprisingly
                   effective at calibrating predictions.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1706.04599"
}


@UNPUBLISHED{Koo2018-cb,
  title    = "Inferring {Sequence-Structure} Preferences of {RNA-Binding}
              Proteins with Convolutional Residual Networks",
  author   = "Koo, Peter K and Anand, Praveen and Paul, Steffan B and Eddy,
              Sean R",
  abstract = "Abstract To infer the sequence and RNA structure specificities of
              RNA-binding proteins (RBPs) from experiments that enrich for
              bound sequences, we introduce a convolutional residual network
              which we call ResidualBind. ResidualBind significantly
              outperforms previous methods on experimental data from many RBP
              families. We interrogate ResidualBind to identify what features
              it has learned from high-affinity sequences with saliency
              analysis along with 1st-order and 2nd-order in silico
              mutagenesis. We show that in addition to sequence motifs,
              ResidualBind learns a model that includes the number of motifs,
              their spacing, and both positive and negative effects of RNA
              structure context. Strikingly, ResidualBind learns RNA structure
              context, including detailed base-pairing relationships, directly
              from sequence data, which we confirm on synthetic data.
              ResidualBind is a powerful, flexible, and interpretable model
              that can uncover cis-recognition preferences across a broad
              spectrum of RBPs.",
  journal  = "bioRxiv",
  pages    = "418459",
  month    =  sep,
  year     =  2018,
  language = "en"
}


@ARTICLE{Shrikumar2017-uj,
  title         = "Learning Important Features Through Propagating Activation
                   Differences",
  author        = "Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul",
  abstract      = "The purported ``black box'' nature of neural networks is a
                   barrier to adoption in applications where interpretability
                   is essential. Here we present DeepLIFT (Deep Learning
                   Important FeaTures), a method for decomposing the output
                   prediction of a neural network on a specific input by
                   backpropagating the contributions of all neurons in the
                   network to every feature of the input. DeepLIFT compares the
                   activation of each neuron to its 'reference activation' and
                   assigns contribution scores according to the difference. By
                   optionally giving separate consideration to positive and
                   negative contributions, DeepLIFT can also reveal
                   dependencies which are missed by other approaches. Scores
                   can be computed efficiently in a single backward pass. We
                   apply DeepLIFT to models trained on MNIST and simulated
                   genomic data, and show significant advantages over
                   gradient-based methods. Video tutorial:
                   http://goo.gl/qKb7pL, ICML slides:
                   bit.ly/deeplifticmlslides, ICML talk:
                   https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.",
  month         =  apr,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1704.02685"
}


@ARTICLE{Simonyan2013-xy,
  title         = "Deep Inside Convolutional Networks: Visualising Image
                   Classification Models and Saliency Maps",
  author        = "Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew",
  abstract      = "This paper addresses the visualisation of image
                   classification models, learnt using deep Convolutional
                   Networks (ConvNets). We consider two visualisation
                   techniques, based on computing the gradient of the class
                   score with respect to the input image. The first one
                   generates an image, which maximises the class score [Erhan
                   et al., 2009], thus visualising the notion of the class,
                   captured by a ConvNet. The second technique computes a class
                   saliency map, specific to a given image and class. We show
                   that such maps can be employed for weakly supervised object
                   segmentation using classification ConvNets. Finally, we
                   establish the connection between the gradient-based ConvNet
                   visualisation methods and deconvolutional networks [Zeiler
                   et al., 2013].",
  month         =  dec,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1312.6034"
}


@ARTICLE{Tremblay2018-ds,
  title    = "{GATA} transcription factors in development and disease",
  author   = "Tremblay, Mathieu and Sanchez-Ferras, Oraly and Bouchard, Maxime",
  abstract = "The GATA family of transcription factors is of crucial importance
              during embryonic development, playing complex and widespread
              roles in cell fate decisions and tissue morphogenesis. GATA
              proteins are essential for the development of tissues derived
              from all three germ layers, including the skin, brain, gonads,
              liver, hematopoietic, cardiovascular and urogenital systems. The
              crucial activity of GATA factors is underscored by the fact that
              inactivating mutations in most GATA members lead to embryonic
              lethality in mouse models and are often associated with
              developmental diseases in humans. In this Primer, we discuss the
              unique and redundant functions of GATA proteins in tissue
              morphogenesis, with an emphasis on their regulation of lineage
              specification and early organogenesis.",
  journal  = "Development",
  volume   =  145,
  number   =  20,
  month    =  oct,
  year     =  2018,
  keywords = "GATA factors; Gene redundancy; Genetics; Human diseases; Lineage
              specification; Mouse development; Transcription",
  language = "en"
}


@ARTICLE{Klemm2019-hk,
  title    = "Chromatin accessibility and the regulatory epigenome",
  author   = "Klemm, Sandy L and Shipony, Zohar and Greenleaf, William J",
  abstract = "Physical access to DNA is a highly dynamic property of chromatin
              that plays an essential role in establishing and maintaining
              cellular identity. The organization of accessible chromatin
              across the genome reflects a network of permissible physical
              interactions through which enhancers, promoters, insulators and
              chromatin-binding factors cooperatively regulate gene expression.
              This landscape of accessibility changes dynamically in response
              to both external stimuli and developmental cues, and emerging
              evidence suggests that homeostatic maintenance of accessibility
              is itself dynamically regulated through a competitive interplay
              between chromatin-binding factors and nucleosomes. In this
              Review, we examine how the accessible genome is measured and
              explore the role of transcription factors in initiating
              accessibility remodelling; our goal is to illustrate how
              chromatin accessibility defines regulatory elements within the
              genome and how these epigenetic features are dynamically
              established to control gene expression.",
  journal  = "Nat. Rev. Genet.",
  volume   =  20,
  number   =  4,
  pages    = "207--220",
  month    =  apr,
  year     =  2019,
  language = "en"
}


@ARTICLE{Park2009-im,
  title    = "{ChIP-seq}: advantages and challenges of a maturing technology",
  author   = "Park, Peter J",
  abstract = "Chromatin immunoprecipitation followed by sequencing (ChIP-seq)
              is a technique for genome-wide profiling of DNA-binding proteins,
              histone modifications or nucleosomes. Owing to the tremendous
              progress in next-generation sequencing technology, ChIP-seq
              offers higher resolution, less noise and greater coverage than
              its array-based predecessor ChIP-chip. With the decreasing cost
              of sequencing, ChIP-seq has become an indispensable tool for
              studying gene regulation and epigenetic mechanisms. In this
              Review, I describe the benefits and challenges in harnessing this
              technique with an emphasis on issues related to experimental
              design and data analysis. ChIP-seq experiments generate large
              quantities of data, and effective computational analysis will be
              crucial for uncovering biological mechanisms.",
  journal  = "Nat. Rev. Genet.",
  volume   =  10,
  number   =  10,
  pages    = "669--680",
  month    =  oct,
  year     =  2009,
  language = "en"
}

@article{tran2020methods,
  title={Methods for comparing uncertainty quantifications for material property predictions},
  author={Tran, Kevin and Neiswanger, Willie and Yoon, Junwoong and Zhang, Qingyang and Xing, Eric and Ulissi, Zachary W},
  journal={Machine Learning: Science and Technology},
  volume={1},
  number={2},
  pages={025006},
  year={2020},
  publisher={IOP Publishing}
}

@article{kuleshov2018accurate,
  title={Accurate uncertainties for deep learning using calibrated regression},
  author={Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  journal={arXiv preprint arXiv:1807.00263},
  year={2018}
}

@article{avsec2020base,
  title={Base-resolution models of transcription factor binding reveal soft motif syntax},
  author={Avsec, {\v{Z}}iga and Weilert, Melanie and Shrikumar, Avanti and Krueger, Sabrina and Alexandari, Amr and Dalal, Khyati and Fropf, Robin and McAnany, Charles and Gagneur, Julien and Kundaje, Anshul and others},
  journal={bioRxiv},
  pages={737981},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}
