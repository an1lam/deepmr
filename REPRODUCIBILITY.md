## Environment setup
### Python
Both our simulation and BPNet experiments require having a working Python experiment set up. Specifically, we ran all experiments using the conda version of Python 3.8.3 on an AWS instance running AWS's "Deep Learning Base AMI (Ubuntu 16.04) Version 32.0" machine image with a Tesla K80 GPU. For completeness, we've included the machine's output from `nvidia-smi` in [the appendix](#nvidia-smi-output).

The first step to reproducing our experiments is cloning this repository. Once you've done that and assuming your environment satisfies these prerequisites, the rest of the environment setup is specific to which experiment you'd like to reproduce.

#### Simulation-specific setup
For the simulation experiment, we've provided a conda environment specification, `src/simulation-conda-requirements.yml`, which you can use to bootstrap a conda environment that should enable reproducing our simulation. To create an environment based off this file, you can run:
```
conda env create -f simulation-conda-requirements.yml
```

This will create a conda environment that you can then activate.

You'll need to install one more package -- [simdna](https://github.com/kundajelab/simdna) -- before you're ready to run simulation experiments. To do so, you can follow the instructions found in the package's README, which (at the time of writing) say to run the following commands in a terminal (with your conda environment activated):

```
git clone https://github.com/kundajelab/simdna.git
cd simdna
python setup.py develop
```

Running our simulation code also requires compiling some Cython code to speed up one-hot encoding of sequences. To do this, run `scr/compile_cython.sh` from the project base directory.

#### BPNet-specific setup
For the BPNet experiment, we've found it easier to install the necessary packages manually since some are direct from Github repositories. In addition to the following instructions, you'll need an environment in which you can run a Jupyter notebook. 

First, you should create a new conda environment and activate it as follows:
```
conda create --name deepmr-bpnet "python=3.6"
conda activate deepmr-bpnet
```

Then, you'll want to install the necessary packages:
```
conda install -c bioconda pybedtools bedtools pybigwig pysam genomelake
pip install tensorflow-gpu==1.15
pip install -e git+https://github.com/kundajelab/DeepExplain.git#egg=deepexplain
pip install scikit-learn==0.21.3
pip install -e git+https://github.com/kundajelab/bpnet.git#egg=bpnet
pip install wandb
pip install --no-deps git+https://github.com/uncertainty-toolbox/uncertainty-toolbox.git
conda install -c conda-forge biopython
pip install torch==1.3.1
pip install ipykernel
```

Finally, you'll want to create a Jupyter kernel to use when you run the two BPNet training and in-silico mutagenesis notebooks:
```
python -m ipykernel install --user --name=deepmr-bpnet3
```

### R
Our actual analysis for the paper used R, in part due to the mature Mendelian Randomization packages being available for R and not Python at the time we were doing the work. All of the R analysis was run on an Ubuntu laptop using R version 4.1.1. For completeness, we've included the output of `sessionInfo` in [the appendix](#session-info-output). All R code can be found in `src/R` including an `install_packages.R` script which should install all (or at least most) of the packages required for reproducing our analyses.

## TF Cooperativity Simulation Experiments
### Generating data
In our paper we discussed four simulation scenarios: no confounding, sequence-dependent confounding, sequence-independent confounding, and both types of confounding. The following command will simulate data for the first scenario using the settings we used for the paper.

```{lang=sh}
python tf_coop_main.py --train_sequences 10000 --test_sequences 1000 --variant_augmentation_percentage .25 --log_summary_stats --data_dir ../dat/sim_e2e --epochs 100 --n_conv_layers 3 --n_dense_layers 2 --seed 42 --train_data_fnames train_labels.csv --train_data_fnames train_variant_labels.csv --model_fname cnn_counts_predictor.pt --model_fname cnn_counts_predictor.pt --model_type ensemble --n_reps 5 --n_rounds 50
```

As noted, this runs without either type of confounding. For sequence-dependent confounding, add `--confounder_motif SOX2_1`. For sequence-independent confounding, add `--confounder_prob .5`. For both, just add both!

### Analyzing simulated data
The two analysis scripts for simulated data are `src/R/compute_sim_metrics.Rmd` and `generate_paper_figures.Rmd`. 

### Computing simulation metrics
`compute_sim_metrics.Rmd` runs the Mendelian Randomization and meta-analysis steps on the data generated by the four simulation scenarios described above. The script assumes that the outputs generated by the simulation can be found in the four directories stored inside of the `sim_results_dirs` variable. In order to reproduce our analyses, you have to either put the data outputs from the simulation run into these directories or change the directory paths to point to whatever locations you've stored the relevant outputs in.

Once you've done that, you should be able to run `compute_sim_metrics.Rmd`'s cells successfully. Doing this will produce tables saved in CSVs that allow the reproduction of the paper's figures.

### Reproducing figures
`generate_paper_figures.Rmd` reproduces the paper's figures. It can only be run after you've successfully run `compute_sim_metrics.Rmd`. Depending on whether you set its `save_plots` parameter to true or false, the script will either just output the figures inline or save them inside of `output_dir` as PNG files.

## BPNet experiments
### Downloading training & validation data
In addition to setting up an environment, running the BPNet experiments from start to finish requires downloading training data. To do so, you can `cd` to whichever directory you want to download the BPNet data to and follow the directions for downloading the data [here](https://github.com/kundajelab/bpnet-manuscript#2-download-the-data).

### Generating data
Producing data that can then be processed by MR and the subsequent meta-analysis requires two steps: training a BPNet ensemble and using it to generate effect sizes and uncertainties for each variant produced by in-silico mutagenesis. These steps live inside two Jupyter notebooks respectively: `src/train_bpnet_ensemble.ipynb` and `src/in_silico_mutagenesis_bpnet_all_tfs.ipynb`. Assuming you've created a kernel for your `deepmr-bpnet` conda environment, you'll now want to start `jupyter` via `jupyter notebook`.

Once you've done this, the next step is to open the `src/train_bpnet_ensemble.ipynb` notebook. Before you run this however, you'll need to update some configuration. Specifically, you'll want to the `.gin` and `.yml` files in `src/bpnet/` and change the relevant paths to point to the directory you've downloaded BPNet data into. Assuming you do so successfully, you can now run the notebook, which will run for a few hours but eventually result in 5 saved CNNs trained on the BPNet data.

From here, you're ready to run `src/in_silico_mutagenesis_bpnet_all_tfs.ipynb`, which will do in-silico mutagenesis and compute effect sizes. Again, here you'll want to update the configuration variables in the second (first non-import) cell to use a valid `output_dir` and `model_base_dir`. From here, you're ready to run this notebook as well. Running it will produce output files which you can then copy to wherever you'll need them to run the MR and meta-analysis steps of the analysis.

### Analyzing data
The analysis script for the BPNet data is `src/R/all_tfs_analysis_bpnet.Rmd`. Before running this, you'll want to update `base_path` in the first non-import cell to point to wherever you've stored the BPNet effect sizes and uncertainties. You also will probably want to change the `save_plots` parameter to `FALSE` or the paths for the output to valid strings (for your computer). Depending on what you specify for `save_plots`, running this script will replicate the figure from the paper and potentially save it as a PNG.

## Appendix
### `nvidia-smi` Output
```
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |
| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

### Session Info Output
 ```
 R version 4.1.1 (2021-08-10)
 Platform: x86_64-pc-linux-gnu (64-bit)
 Running under: Ubuntu 20.04.1 LTS (fossa-bulbasaur X38)

 Matrix products: default
 BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
 LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

 locale:
  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
 [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

 attached base packages:
 [1] stats     graphics  grDevices utils     datasets  methods   base     

 loaded via a namespace (and not attached):
  [1] mclust_5.4.7                 Rcpp_1.0.6                  
  [3] mvtnorm_1.1-2                lattice_0.20-44             
  [5] tidyr_1.1.3                  assertthat_0.2.1            
  [7] glmnet_4.1-1                 digest_0.6.27               
  [9] foreach_1.5.1                utf8_1.2.1                  
 [11] gmp_0.6-2                    R6_2.5.0                    
 [13] MatrixModels_0.5-0           pracma_2.3.3                
 [15] evaluate_0.14                httr_1.4.2                  
 [17] ggplot2_3.3.4                pillar_1.6.1                
 [19] iterpc_0.4.2                 rlang_0.4.11                
 [21] lazyeval_0.2.2               minqa_1.2.4                 
 [23] rstudioapi_0.13              data.table_1.14.0           
 [25] SparseM_1.81                 nloptr_1.2.2.2              
 [27] Matrix_1.3-4                 mathjaxr_1.4-0              
 [29] rmarkdown_2.9                splines_4.1.1               
 [31] lme4_1.1-27                  htmlwidgets_1.5.3           
 [33] munsell_0.5.0                compiler_4.1.1              
 [35] MendelianRandomization_0.5.1 xfun_0.24                   
 [37] pkgconfig_2.0.3              shape_1.4.6                 
 [39] arrangements_1.1.9           htmltools_0.5.1.1           
 [41] tidyselect_1.1.1             tibble_3.1.2                
 [43] codetools_0.2-18             matrixStats_0.59.0          
 [45] fansi_0.5.0                  viridisLite_0.4.0           
 [47] crayon_1.4.1                 dplyr_1.0.7                 
 [49] conquer_1.0.2                MASS_7.3-54                 
 [51] grid_4.1.1                   nlme_3.1-152                
 [53] jsonlite_1.7.2               meta_4.18-2                 
 [55] gtable_0.3.0                 lifecycle_1.0.0             
 [57] DBI_1.1.1                    metafor_3.0-2               
 [59] magrittr_2.0.1               scales_1.1.1                
 [61] KernSmooth_2.23-20           cli_2.5.0                   
 [63] robustbase_0.93-8            xml2_1.3.2                  
 [65] ellipsis_0.3.2               generics_0.1.0              
 [67] vctrs_0.3.8                  boot_1.3-28                 
 [69] rjson_0.2.20                 iterators_1.0.13            
 [71] tools_4.1.1                  CompQuadForm_1.4.3          
 [73] glue_1.4.2                   DEoptimR_1.0-9              
 [75] purrr_0.3.4                  ks_1.13.1                   
 [77] survival_3.2-12              yaml_2.2.1                  
 [79] colorspace_2.0-1             plotly_4.9.4.1              
 [81] knitr_1.33                   quantreg_5.86         
 ```
